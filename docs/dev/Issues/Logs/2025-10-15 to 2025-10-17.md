|                       |        |
| --------------------- | ------ |
| SmokeTest_ProcessTADS | FAILED |
SmokeTest_ProcessTADS
No such variable: resolution -- Check script '/home/benjamin/Documents/hich/contents/subwf/features/TADs/functions.nf' at line: 23

tad_opts.resolution is what we're looking for.
we're passing in a resolution directly rather than a map
Also, it's not clear what a smoke test is -- stub run or just calling it and seeing if it runs without any guarantees?

SmokeTest_AlignerIndex_Full FAILED
`[bwa_idx_build] fail to open file 'hg38_chr19.fa.gz' : No such file or directory`

Figure out solution for downloaded or locally generated resources (references, indexes)
Can we create a small reference, i.e. with just the smallest contig from one chromosome or an entire very small genome?
+ [M129 reference genome (250kb after gzip)](https://www.ncbi.nlm.nih.gov/nuccore/NC_000912.1?report=fasta)

Ideally should have real Hi-C data, i.e. from [Trussart 2017](Paper/Citations.md)
+ Uses M129 reference genome
+ [FASTQ download site](https://www.ebi.ac.uk/biostudies/ArrayExpress/studies/E-MTAB-3721/sdrf)
	+ About 9G per sample (2x4.5G for paired-end fastq files), so would need to be downloaded and extracted.
	+ HindIII or HpaII digest 
+ Extracted first 1k reads from the first listed sample

Would like to replace my hg38/mm10 based alignment tests with this, but it's not methylated. Does that matter though? We can just use bwameth anyway. Replaced index and alignment tests with M129-based tests and updated .gitignore so it now tracks tests/assets/index. Test is now 7s instead of several minutes.

Next step should be to come up with an overarching test strategy.
+ Comprehensive pipeline, configuration, profile testing
+ Scalability
	+ Individual processing steps
	+ Stub run with very large numbers of samples (i.e. 100k-1M)
+ Find some way to validate the outputs

All nf-test tests currently pass.

Added script create_stub_samples.py to generate large sample files to validate orchestrator scalability. On my desktop PC, a stub run of 10,000 samples takes 6.5 minutes. These have to be manually generated as the biggest ones are too large to put on github (>100MB).

Deduplication parameters are not set using the standard `_opts` map-based method. Instead, samples have `dedupSingleCell`, `dedupMaxMismatch`, `dedupMethod`, and `pairtoolsDedupParams` attributes used as input to DEDUP_PAIRS.

By contrast, other processing steps have parameters like `align_opts`, `ingest_pairs_opts`, `parse_to_parse_opts`, and so on. By extension, deduplication should have `dedup_opts`.
+ dedupMethod is mentioned only in aggregate/DedupPairs/workflow.nf, the test for this workflow, and in the vignettes.
+ dedupMaxMismatch is mentioned in these plus aggregate/LabelAggregationPlans
+ dedupSingleCell is untested but otherwise mentioned in the same places as dedupMethod
+ pairtoolsDedupParams is mentioned in aggregate/DedupPairs in functions.nf, process.nf, and workflow.nf as well as in preconfigured vignette defaults.

# Projects
+ Formal test plan
+ Migrate test assets to git lfs
+ Replace end-to-end test with M129
# Changes
+ Matched SmokeTest_ProcessTADS to match updated process interface
+ Added the ~500kb M129 genome to genomeReference resource to test indexing at `assets/referenceGenome/M129.fa.gz` and updated SmokeTest_AlignerIndex_Full to use it instead of hg38_chr19. Test now runs in 4s. Therefore, removed stub run test as it'll hardly be faster than the full run. WARNING: Still using human data that doesn't match the index for this test, which could be deceptive. 
+ Created tool to extract first N lines from gzipped files downloaded with curl to create small test datasets
+ Added 1k-read paired fastq files from Trussert et al.
+ Added governance policy for citations
+ Replaced alignment smoke test with M129-based data and indexes